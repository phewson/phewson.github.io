[{"content":"\nNowadays, we rarely ask the question “do we have data”. Often we have lots of data and the first barrier we hit may force us to instead ask “do we have the right data?” Even when the answer to this question is NO, we often need to be able to do the best we can with the available data. It has become so easy to collect vast quantities of electronic data as a byproduct of other processes that we expect data to be on tap and free. It is still the case that well designed and specific data collection can yield better answers than a “free” data source. However, we still need to be able to combine insights from different data sources; some purposefully collected, some byproducts of another process. This blog will illustrate the caveats and cautions we have to take in order to learn from data.\n(I would have used the acronym AI for Actionable Insight but that acronym has been taken)\n","title":"Actionable Insight","uri":"/sections/insight/"},{"content":"There are two key things to say about modern Data Science.\nContinuous Improvement / Continuous Delivery The first concerns “Continous Integration/Continuous Delivery” (CI/CD); a modern software development practice. The advantages of scripted analyses (reproducibility, testing, auditability) are well established; it’s a small leap to seeing these scripts as simple pieces of software and realising they need their entire architecture specified and tested continuously. Indeed I have published in Annals of Operations Research on the pitfalls of spreadsheets for data analysis. Using CI/CD in software projects is a well established way of minimizing technical debt; in data science there is a human tendency to focus on producing beautifully crafted scripts that work well today, without considering all the various stages of analysis of software dependency. One example, we need to update software versions regularly (bugfixes and needed feature enhancements).\nModern Statistical Methods The second concerns an up to date knowledge of what is possible in statistical science. The field of statistical science is being advanced just as rapidly as any other science. Many statistics courses are based on techniques which were invented before 1930, and training has not been updated. Given the availability of powerful computers, convenient analytical methods are no longer needed to approximate a solution. We can look for the right solution, if we allow a little computer time. Given the price of computer time, this rarely seems like a barrier to using the best methods avvailable. There is old adage in English “if the only tool you have is a hammer, then every problem you see is a nail”. This is the problem with statistical training. We are only given the hammer, and a hammer from the 1930s at that. This blog will provide case studies of using modern methdods to answer real questions. Yes, this does require deeper statistical expertise, but in turn it requires domain specialists to think harder about the problem they want to solve, rather than just “hitting it with a hammer”\n","title":"Modern Data Science","uri":"/sections/modern/"},{"content":" Photo Credit: Terry Calcott\nThis was a cold paddle, with a little bit of wind coming from the North. We started off as a single group with two paddlers on their first Saturday paddle. At Fort Picklecombe, the leader decided to split us into two groups; one going to Mount Edgcumbe for lunch and the other to Cawsand. As it turned out, we went a little beyond Penlee point and had lunch at a beach (Sandway’s beach?) before returning home.\nPhoto Credit: Terry Calcott\nWe did the compulsory rock-hopping as we went. Even though there’s very little swell, I got stuck on rocks at Fort Picklecombe; managed to extricate myself rather clumsily. I did a little more rock hopping near Penlee Point as well as the north side of the bay returning from lunch. I had problems selecting a good point to travel over low rocks as well as sometimesd getting pushed sideways. I think this all fits under the idea of being “bossed” by the sea when I need to do things with more power and less finesse. Popping outside of Penlee Point was wonderful, there looks like a great paddle over to Rame. I found the choppy swell by Penlee harder to deal with than the big slow swell we had a couple of weeks ago, but I seem to be getting more comfortable with it.\n","description":"Paddling on a cold, calm day","tags":["Sea-Kayak","Port-of-Plymouth-Canoe-Association"],"title":"Drakes Island and beyond Penlee Point","uri":"/kayaking/2025-02_08_outside_penlee/"},{"content":" Again, this uses data released by the Home Office on Fire and Rescue involvement in Road Traffic Collisions. These data are in very aggregated form, and not entirely useful for analysis. However, I’m just trying to understand the data available before I think harder about what questions you might want to ask. There is one interesting table on extrications by Fire and Rescue Service. I’ve summed the numbers for the last three years and used some ONS mid-year population estimates to create a rate.\nIt’s also interesting to look at this on a per-capita basis, using mid-year population estimates for each local authority aggregated to Fire and Rescue Service areas.\nSo this would imply that North Yorkshire has a high per capita rate of road collisions that require support from the Fire and Rescue service. One other comment is that Devon and Somerset have high numbers of Roof Flap and Side Removal incidents; these are not entirely explanained by per-capita rates.\nSome of the todos that arise from this\nI need to find out why I’m not getting a population for Cumbria Fire and Rescue service I’d like to look at the trends over time here, I’m sure they have a powerful story to tell. I’ve a lot to do cosmetically with the maps (it was quite difficult having each map with it’s own z axis scale/legend. I ended up using the patchwork package to create each map separately and re-assembled them with wrap_plots(). ","description":"Mapping extrications","tags":["stats19","road-collisions","fire-and-rescue"],"title":"Fire and Rescue RTI extrications","uri":"/posts/2025_02_07-fire-and_rescue-rti-extrications-mapped/"},{"content":" This also uses the Home Office on Fire and Rescue involvement in Road Traffic Collisions. Again, these data are aggregated to the point they are difficult to use. But by way of understanding what’s here, I’ve tried looking at an “Extrication rate”. For each sex and age grouping, I’ve calculated the number of STATs19 reported serious casualties and used this as a denominator.\nNow, this requires so much more checking and other work needed. But the implication is that female casualties are more likely to need an extrication than a male casualty. At first glance that is a little counter-intuitive and implies a whole lot more work to figure out what I might have missed here.\n","description":"Extrications by age/sex","tags":["stats19","road-collisions","fire-and-rescue"],"title":"Fire and Rescue RTI extrications by age/sex","uri":"/posts/2025_02_08-fire-and-rescue-extrications-age-sex/"},{"content":" Now, it’s important to note we are talking about very different things here. In theory all vehicular road collisions in which someone is injured should be reported to the police. Other services are only involved when the injuries are serious enough. But, at an aggregate level you would expect some kind of association between the number of incidents reported to the police and the number attended by the Fire and Rescue service. I’ve recently had my attention drawn to some of the data released by the Home Office on Fire and Rescue involvement in Road Traffic Collisions. Unlike the STATs19 (Police Recorded Data) it’s very aggregate, but it is possible to make some comparisons. The following are based on Table FR904 which details the number of fatalities and casualties attended. We can do the comparisons fairly simply from 2016\nSTATs19 contains a Local Authority District (LAD) field There are lookups that relate LAD to Fire and Rescue Service from 2016 We can therefore count the number and type of STATs19 record in each given Fire and Rescue service area from 2016. There are data back to 2010 from the Fire and Rescue service, but there wasn’t an off-the-shelf lookup; so that’s going to take a little more time.\nFirst of all, we can just consider looking at the number of fatalities reported by STATs19 in each Fire and Rescue Service area and the number of fatalities they themselves reported they attended.\nIt’s also interesting to look at this on a per-capita basis, using mid-year population estimates for each local authority aggregated to Fire and Rescue Service areas.\nAnd we can do something similar for casualties. I had hoped to look at the more serious casualties only among the Fire and Rescue Service data, but there are three authorities count all casualties and unknown.\n","description":"Comparing police reported and fire and rescue reported road collisions","tags":["stats19","road-collisions","fire-and-rescue"],"title":"Fire and Rescue reporting of RTIs","uri":"/posts/2025_02_05-fire-and_rescue-reporting-rti/"},{"content":" Photo Credit: Paul Hewson\nThis was an interesting day, sandwiched between outburts of Storm Éowyn saw very little wind, but some large low frequency swell. My weather API suggested 2m; it was hard to believe it was that big. However, it was low frequency, and I didn’t get far enough away from anyone to “calibrate” my call on this. Indeed, we managed to paddle across the sound outside the breakwater.\nPhoto Credit: Paul Hewson\nWe did a little rock hopping on the east coast of the Sound before crossing. Rather than ask if we wanted to paddle outside the breakwater, Adam asked us to rate our comfort levels from 0 to 5, and pointed out the possibility of hiding at the end if we needed to. We then went to have a closer look at Penlee Point on the basis it was relatively well sheltered on the west of the Sound. I did a bit more rock hopping and was surprised by the swell, I had been planning a nice bow rudder manoeuvre but ended up with an ugly handbrake turn and a lot of back paddling. Some paddlers went and had a look at the big swell just round the corner. I had to decline as I needed the loo. We couldn’t land on the small beach at Penlee Point so went back to Cawsand. We had some home made tablet, I got coffee from the shop and we listened to some bagpipe music.\nPhoto Credit: Paul Hewson\nThe paddle back was fairly uneventful. I do find following swell more challenging, and I think there was some bounceback from Fort Picklecombe.\nPhoto Credit: Paul Hewson\n","description":"Paddling with a large but calm swell","tags":["Sea-Kayak","Port-of-Plymouth-Canoe-Association"],"title":"Penlee Point and Cawsand","uri":"/kayaking/2025-01-25_penlee_cawsand/"},{"content":" There are a few oddities like this lurking in the collision data that deserve more of an explanation. There are (sadly) a small number of pedestrians who are killed following a collision with a bicycle most years. But what caught my eye was the way that the number of serious injuries has been increasing over the last few years. This feels like something that deserves a better explanation.\n","description":"Increasing numbers of pedestrians seriously injured in cycle collisions in recent years","tags":["stats19","road-collisions","pedestrians"],"title":"Pedestrian casualties of cycle crashes","uri":"/posts/2025-01-22-pedestrian-casualties-of-cycle-crashes/"},{"content":" Data have been obtained from Department of Energy Security and Net Zero Electricity data is divided between domestic and non-domestic categories according to the meter’s profile class. Domestic consumption is based on Non-Half Hourly (NHH) meters with profiles 1 and 2 (these are the standard domestic and economy 7 tariffs respectively). Non-domestic consumption is based on NHH meters with profiles 3 to 8 and all Half-Hourly meters. In addition, profile 1 and 2 meters are re-allocated to the non-domestic sector if their annual consumption is greater than 100,000 kWh; or if their annual consumption is greater than 50,000 kWh and the address information for meter suggests that it is non-domestic. There are some data quality issues documented in the Subnational methodology and guidance\nWell.\nLooking at the underlying data as a map\nHowever, aggregated to LAD level:\n","tags":["Energy","Transition","Carbon-Footprint"],"title":"Patterns in energy usage in England and Wales","uri":"/posts/2025-01-14_energy_usage_england_wales/"},{"content":" Photo Credit: Fiona Hiscox Tracker\nPhoto Credit: Paul Hewson\nPhoto Credit: Paul Hewson\nPhoto Credit: Paul Hewson\nIt took about 90 minutes to arrange the car shuffle; most cars parked in Kingswear with the bare minimum returning to Totnes. We embarked from Longmarsh Car Park and followed the tide down the river. Weather was extremely cold, and it was a little windy, especially turning after Dittisham where the Dart widens out.\nWe did see a single seal, then near East Cornworthy a group were more curious as to who we were.\n","description":"Estuary paddle with the tide","tags":["Sea-Kayak","Teignbridge-Canoe-Club"],"title":"Totnes to Kingswear","uri":"/kayaking/2025-01-04_totnes_kingswear/"},{"content":" Photo Credit: Helga Pinn Facebook\nThis was a cold, dark and very flat day. Nice paddle though. Over 12 miles. Did some rock hopping in unchallenging conditions.\n","description":"Cold flat paddle to Wembury","tags":["Sea-Kayak","Port-of-Plymouth-Canoe-Association"],"title":"Mountbatten to Wembury","uri":"/kayaking/2024-12-28_mb_wembury/"},{"content":" Photo Credit: Lindsey McPhee Facebook\nI volunteered to act as a “Guinea Pig” for a young instructor taking his Sea Kayak Leader award. The nice thing here is that I have to fall in the water, so there’s no shame if it happens. The plan had been to paddle from Salcombe, but that was rearranged.\nI’m not completely sure how much swell there was, I suspect round Thatcher Rock it might have been exceeding 1.5 meters. I’m still finding turning (edging) in that kind of swell non-trivial. But overall I really enjoyed the session; and I know I can do a heel hook in those conditions.\n","description":"Very weather challenged session from Meadfoot","tags":["Sea-Kayak"],"title":"Meadfoot Guinea Pig","uri":"/kayaking/2024-11-02_meadfoot_guinea_pig/"},{"content":" Photo Credit: Paul Hewson screenshot\nThis was a short but challenging paddle from Teignmouth to Maidencombe. I think the plan had been to stop in for coffee and then move on to Babbacombe for lunch, but there was a fair bit of swell. Andy Benham suggested it was closer to 1m than the 0.8 shown on my forecast.\nI didn’t get any photos, but the launch was a bit interesting (no getting in your boat while afloat, the surf meant doing up the spraydeck and scraping down the last of the beach).\n","description":"Short weather challenged paddle from Teignmouth to Maidencombe","tags":["Sea-Kayak","Teignbridge-Canoe-Club"],"title":"Maidencombe Paddle","uri":"/kayaking/2024-10-12_maidencombe/"},{"content":" From Pexels Karolina Grabowska\nI’ve been using emacs for a long time. OK, I’ve probably been using emacs for over 30 years. However, I’ve never really taken a huge amount of time to learn emacs. I’ve picked up a few packages along the way\nESS (Emacs Speaks Statistics) and various *-polymode packages magit org-mode and more recently org-roam I’ve also spent time adjusting a development environment as necessary, which most recently has included compiling from source so that I can look at the tree-sitter parser. But it was really recent attempts to find accessible settings for emacs that lead me to the inspired System Crafters website and [YouTube](https://www.youtube.com/c/systemcrafters “System Crafters YouTube channel) channel.\nI have at times experimented with different completion frameworks in emacs, but never found them easy to use, they got in the way more often than they helped. They may or may not save a few keystrokes, but I am not able to type faster than my brain can parse information. Anyway, I’m delighted to find a new completion framework in vertico which as far as I can see works for me because it’s so well integrated into the basic emacs auto-completion, so it doesn’t add a new and distracting layer on top of your work. From what I understand, the UI I’m seeing also reflects some elegant coding to build it into emacs core. I’m using a fairly vanilla configuration; for example no cycling and no memory (recent completions first). Again, the tradeoff between the convenience where you want to do what you did last time versus looking for something obscure means I’d rather have a little bit more scrolling over the familiar than a brain freeze over wondering why I can’t see anything to do with the really old work.\nWhat impressed me most though was marginalia, the addition I didn’t realise I needed. It adds some meta-data to the auto-completion (e.g., file size, last opened for files) which is surprisingly helpful. Especially when I’m hunting for old files.\nThere’s way more on the System Crafters website,\n","description":"emacs: vertico and marginalia packages","tags":["emacs","autocomplete","configuration"],"title":"Enhancing emacs: vertico and marginalia packages","uri":"/tech/2024-05-28-emacs_vertico_marginalia/"},{"content":" Photo by Cameron Readius from Pexels\nI can’t pretend I had read Rebecca Thomson’s article “Bankruptcy, prosecution and disrupted livelihoods – postmasters tell their story” in May 2009 in Computer Weekly, but certainly through journalism such as Private Eye I was aware well before the ITV drama screened in 2024.\nTherefore, I’m most surprised by the evidence presented by Paula Vennells at the Statuary Inquiry on May 22nd. I don’t want to get into an appraisal of her credibility or her competence, other than to wonder how much media coverage you can dismiss and still do your job. I merely wanted to pick up on the fact she claimed she didn’t have the management information to make her aware of the problem. I would like to take that at face value; I can appreciate running a large and complex business means that you can’t be over all the minutiae. Paula Vennells claimed she couldn’t be expected to be aware of single postmaster disputes. Which implies, on reflection, she thinks there should have been performance indicators such as “the number of postmasters in dispute”, “the value of money disputed” and so on. The trouble with this suggestion, is that we now know, with the benefit of hindsight, that these are the kinds of indicators that need monitoring. If we were going to have indicators covering “everything a CEO might need to know”, wouldn’t we end up in the same place; too many indicators for a CEO to monitor. Much as I love the idea of building lots of dashboards to give better visibility to the functioning of a business, I do wonder if the K in KPI is important. It’s a Key Performance Indicator.\nI will be following the outcomes of the Enquiry with a lot of interest now, to see what is made of this suggestion. One of the other reflections I have is that Data Analysis is moving away from Dashboarding and more into Data Stories. That seems to encourage the kind of information filtering that Paula Vennells claims was the problem here. Data Stories means you tell the top tier what they want to hear, and don’t bog them down with irrelevant details (like bugs in software).\n","description":"The role of data in governance","tags":["Key-Performance-Indicators","Data-ethics-and-governance","Corporate-governance","KPI"],"title":"Reflections on the Post Office Horizon Enquiry","uri":"/posts/2024-05-23-post_office_horizon/"},{"content":"I had partitioned my development environment such that I had virtual machines providing:\nR and R-studio A PostgreSQL server A development machine This basically avoided some package clashes when “tinkering”. There is however some clumsiness (I need lots of PostgreSQL clients all over the place). So the current cunning plan is to run some of this inside Docker.\nThe first step was adding a GPG key and the docker repositories. I seemed to faff around a bit but I think you just sudo apt-get install docker-ce (I think I tried to install too many small parts, and had to add facilities to enable docker compose up and could have avoided all this if I’d just installed docker-ce). It’s probably worth checking the current installation pages to do this properly.\nThe second thing I’d forgotten was that I needed to set up groups and permissions sudo usermod -aG docker ${USER} and then log in to be able to use the group.\nFinally, when I ran docker compose up it turns out I needed to have already run docker build. I guess this is likely a shortcoming in my compose.yaml. I’d also forgotten that I needed my own .Rprofile under my configuration (although I’ve pre-specified most of the R packages it’s handy to be able to persist any ad hoc installations. I think. Maybe it’s safer not to have that option and to force yourself to only install packages through the installation process).\nAnyway, I have a working R installation isolated in a docker container. I can access R directly on the command line via docker exec -it rstudio_paul /bin/bash, but don’t currently get any graphics devices - maybe I need to set it up as an ssh server and access via TRAMP mode on emacs. I think that means running something like the following:\n14 15 16 17 18 19 20 21 RUN apt-get update \u0026\u0026 apt-get install -y openssh-server RUN mkdir /var/run/sshd RUN echo 'root:root123' | chpasswd RUN sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config EXPOSE 22 CMD [\"/usr/sbin/sshd\", \"-D\"] ","description":"Using Docker","tags":["R","Docker"],"title":"Using Docker for R","uri":"/tech/2024-05-19-rocker/"},{"content":" Photo by Pixabay from Pexels\nI’ve been reading a lot about how Differential Privacy is the answer to all our privacy concerns. However, having probed the methodology in detail, I’m not so sure. Indeed, I think we have a silo problem. Differential Privacy has been developed in a Computer Science framework as a means of balancing the need for altering data to meet privacy requirements with the need to maintain data close enough to their raw state to yield meaningful answers. But I’m not convinced people working with tools from canonical Computer Science ask the same questions as people working from canonical Statistical Science (I include sociologists, demographers, public health researchers within this latter framework). So I’ve written this post as a first step to understanding the broader Differential Privacy framework. I can understand specific results for specific methods; but I just don’t see that Differential Privacy magically solves all possible trade-offs between preserving individual privacy and making data available to researchers which can answer their questions with sufficient accuracy.\nI also have my own bias, which is that we should avoid bias. That’s a slightly strange statement from a Bayesian Statistician who is stereo-typically willing to accept a little bias if the estimation method has better mean square error (for want of a better metric). In this case, what I mean by bias is that we don’t want to use privacy methods that systematically underestimate the strength of relationship between variables. I fully respect the need to preserve individual privacy. Indeed, I look to governments and census bureaus to lead the way in showing how you can respect privacy and obtain insight from data. Nevertheless, I would prefer we used methods which were able to account for any uncertainty we have introduced into an analysis as a result of using a privacy respecting stage in our data pipeline..\nI have delivered many consulting problems where I needed to work with publicly available data that had been subject to some disclosure control mechanism. A well established weakness of many applications of statistical methods is that they only provide uncertainty estimates (confidence / credible intervals) for aleatory uncertainty; that part of our uncertainty due to random sampling. Epistemic uncertainty such as non-response bias, model mis-specification or data that have been permuted for privacy reasons is simply ignored. I have never liked the fact that disclosure control methods introduce uncertainty in a way that cannot be acknowledged in the results of an analysis. To give a concrete example, you don’t want to do an analysis comparing a policy intervention with a baseline and report that the credible intervals for the effect of the intervention did not overlap if you knew that adding in an allowance for various sources of epistemic uncertainty widened the intervals such that they did overlap. You end up recommending policies that are not effective.\nAs it happens, most of my engagement with this subject has been in trying to reconstitute multi-way census tables. I know full well this is impossible (because the disclosure control methods make it so). Equally I know there are methods such as Bonferroni bounds which let you put bounds on the range of count values that any given cell could take (strictly speaking these should be called Bonferroni-Fréchet-Hoeffding bounds as several workers discovered them simultaneously). But for myself, I’ve always preferred the idea of releasing multiple versions of a table which allow propagation of the uncertainty after you’ve applied a method such as iterative proportional fitting to reconstitute the full table based on the released micro-data and local census tables.\nThe long and the short of this problem is that there is a trade-off between protecting individual privacy and preserving the statistical accuracy for researchers. The more noise you add, the less likely that individuals can have their data identified. But the more noise you add, the further the data are from the “truth”. The big problem for working with tables is that permuting the cell contents may weaken the observed relationship between variables.\nThere are two “classical” methods for ensuring data privacy. The first is cell suppression. You never allow any query if the conditions of that query are met by only one record in the database. The problem with this rule is that you can attack it using set differencing. I can’t get a cross-tabulation of age, sex and commute method because there is a single female cyclist in a given age band in a given census output area. So I query how many female cyclists there are in that output area for all ages, and then for all ages excluding the one of interest and can see there is a single respondent. A popular alternative is data swapping; a subset of the data is taken and within this records are swapped with that of similar records. So I may think I have identified an output area with a single female cyclists of a given age band; the truth is I don’t know whether she was swapped there from another area. At a higher level, the numbers are consistent, but as my querying becomes more granular the noise may be a bigger component of the table. There is a well established R package which implements classical disclosure control methodology sdc. The ONS provides extensive guidance on how to apply disclosure control methods and a common method was chosen for 2011 for all UK census authorities.\nAnyway, life moves on and of course state of the art in ensuring data privacy when releasing data nowadays seems to be Differential Privacy. DiffPriv provides differential privacy methods in to complement the more traditional disclosure control methods. Despite being the current fashion, it is an old and widely used technique. Just add some random noise. For researchers with simple univariate analyses to conduct, the Data Privacy modified data should have an average value should be close to the true value. Unlike data swapping however, noise is typically added to the final value rather than to the raw data. Also, unlike traditional disclosure control methods, queries are run repeatedly on the source data and different perturbations may be applied each time. With more conventional disclosure control, a perturbed set of results is created once and then released to the public. Moreover, under Differential Privacy, a perturbation method must be developed for every statistical method. With more traditional disclosure control, once the data had been perturbed they can be used for any kind of analysis.\nDifferential Privacy is defined in probabilistic terms: Consider two datasets \\(DF\\) and \\(DF’\\) Both have the same variables \\(j=1, \\ldots, P\\) and the same number of row \\(i=1, \\ldots, n\\) However, they differ in the contents of one row Denote the response to applying a query \\(Q\\) to these datasets as \\(Q(DF)\\) and \\(Q(DF’)\\). For any set \\(\\Omega\\) that can be created by applying \\(Q()\\) to any such dataset,\n$$P(Q(DF) \\in \\Omega) \u003c \\exp(\\epsilon) \\times P(Q(DF’) \\in \\Omega)$$\nThe smaller we set \\(\\epsilon\\) the more we assure privacy.\nAn example (taken from the excellent Differential Privacy: A Primer for a non-Technical Audience is as follows:\nConsider computing an estimate of the number of HIV-positive individuals in a sample, where the sample contains $n= 10,000$ individuals of whom $m= 38$ are HIV-positive. In a differentially private version of the computation, random noise $Y$ is introduced into the count so as to hide the contribution of a single individual. That is, the result of the computation would be $m′= m+Y= 38 +Y$ instead of $m= 38$. To me, it seems we have several remaining problems. These include the use of Laplacian errors to perturb count values. This means if your Laplacian noise makes a count negative you then need to set that equal to zero which adds an additional step and hence further noise. But I also note that we are talking about univariate analyses. I’ve been almost entirely concerned about the associations between categorical variables (using a variety of log-Linear, graphical models or recently graphical causal inference models to provide analytic output). This kind of noise addition is going to reduce the apparent association between categorical variables in published tables. Given my interests tend to be on the lines of “what is the association between ethnicity and Covid-19 mortality, conditional on a number of important other factors such as age, sex, occupation and so on”, I really don’t want to be given data subject to a Differential Privacy technique which reduces the strength of that association. And ideally, I would like to be able to estimate the epistemic uncertainty associated with the data privacy perturbations. We accept random sampling in statistics; and quantify the associated errors. Why aren’t we looking for privacy preserving methods that help us quantify the uncertainty we’ve added to an analysis in doing this?\nIn summary, I still think I like the idea of being supplied multiple tables of results in a way that preserves privacy but lets me quantify the uncertainty associated with these kinds of procedures. This gets talked about constantly in statistical circles. I have to say, I’m not convinced Differential Privacy is the last word on the matter.\n","description":"The role of disclosure control and differential privacy in data privacy and statistical analysis","tags":["Disclosure-control","Differential-privacy","Data-ethics-and-governance","Public-data"],"title":"Data Privacy; Disclosure control and differential privacy","uri":"/posts/2021-08-05-disclosure_control_privacy/"},{"content":" From Pexels Wendy Wei\nI do have one little problem with sqitch. I am so used to typing a letter u after a letter q that I regularly mistype the command as squitch, and then end up staring at the terminal for so long I wonder if I should have typed sqint. Very minor naming issues aside, the project has a website at www.sqitch.org. I’ve found it a simple and specific database migration tool which does as much as I want and no more. It’s written in Perl and available directly from Ubuntu repositories with\nsudo apt-get install sqitch sqitch is designed to work very closely with git. Indeed, as with using git init to create a new repository, executing sqitch init in a git repository will initialize a data managed migration project. This blog post documents a couple of departures I made from the official tutorial. Optional arguments to the init call give the name of the project, a remote git repository address and the database engine of choice. A full call to init would therefore contain the following information:\nsqitch init \u003cMY PROJECT\u003e --uri \u003cMY REMOTE GIT REPO\u003e --engine pg This will generate two local files, sqitch.conf and sqitch.plan. These files need to be kept under version control, along with the sql files that will be generated under three folders, deploy, revert and verify. The manual recommends adding information on the location of the local psql as well as a user name and email address.\nsqitch config --user engine.pg.client /usr/bin/psql sqitch config --user.name 'Paul Hewson' sqitch config --user.email '\u003cMY EMAIL ADDRESS\u003e However, this has the effect of adding a “global” config file in ~/.sqitch/sqitch.conf. I find this annoying partly because I like the idea of keeping dotfiles under ~/.config. More importantly, I prefer keeping all config files for a specific project under the folders for that project and hence under the version control for that project. I would rather my project git history knew if I had swapped psql locations at some stage. One change I found I had to make manually was to specify the target databases. This means adding a section to the local sqitch.conf file. The name “official_surveys” becomes a proxy for the full database server specification. The preamble db:pg indicates that this is a PostgreSQL database. This makes is simple to specify test and production databases as necessary. Currently, my project is a research project working with nationally commissioned surveys and I only have one working database. I may in due course decide to run a “production” database in a container.\n[target \"official_surveys\"] uri = db:pg://USERNAME@HOSTNAME:PORTNUMBER/DATABASE_NAME Having set up the project, the first step is to add files locally specifying the DDL required for the first migration. sqitch add with an optional (short) comment sets up the required sql files locally and makes an adjustment to the sqitch.plan.\nsqitch add first_migration -n 'My first migration' This generates files in three folders, namely\ndeploy/first_migration.sql revert/first_migration.sql verify/first_migration.sql Here’s where the fun starts. The next task is to write into these files all the DDL necessary to set up schema, tables, views and functions in the deploy script, all the corresponding DDL to remove these objects in the revert script. The verify script is intended to provide tests that the database structure is correct. In a complex migration scheme on a live database there are lots of ways this could go wrong. In my case, I am somewhat misusing this by writing anonymous scripts to ensure that data has been loaded correctly into the tables. This is an illustration from the National Travel Survey. Data is supplied in a tab separated value, with the column name HomeIUse_B01ID and the responses are coded as integers, including -8, -9 for missing data values. This is manipulated into an enum in the target table. There are therefore several steps where something could go wrong, and so a number of variables are chosen at random and checks are made that the data has been transformed correctly.\nDO $$ DECLARE home_internet int; BEGIN home_internet := ( select count(*) from dft.nts_individual_2002_2019 where internet_frequency_home::text = 'Several times a day' ); ASSERT home_internet = 25388; END $$; Running the command\nsqitch verify official_surveys will execute these functions on the target database. As noted, this is an abuse of the design intentions of sqitch as the command could be executed along with the deploy script as\nsqitch deploy --verify official_surveys would both deploy a migration and verify the subsequent database structure. However, this slight misuse of sqitch serves a purpose at present until I find a way to more closely link data import steps to data verification steps.\nDeveloping a new migration consists of repeatedly cyling through\nsqitch deploy official_surveys and\nsqitch revert official_surveys --to @HEAD^ -y until the migration looks acceptable, fixing any errors which arise. At that stage, I can import the data and verify the migration and data transformation. One point of note is that the --to @HEAD^ reverts the last migration only (i.e., the migration currently being worked on). It is possible to use alternatives such as @HEAD^^ and @HEAD^3 which remove all migrations up to the immediately previous one, and two migrations previous. @ROOT may be a great England cricketer, but reverting --to @ROOT will remove all migrations. It is possible to be more precise; by issuing:\nsqitch log official_surveys we obtain a log of all migrations applied to the target database, with the SHA for each migration and can use the SHA codes to limit the reversion.\nFinally, two features I haven’t used yet. At a certain point, it is possible to tag a migration using\nsqitch tag v1.0.0 -n 'A useful comment' and there is also a sqitch bundle command which bundles all the migrations in a bundle folder ready for zipping and distributing. I can’t see my self every using this, as I can only imagine using sqitch from a git repository.\nAnyway, I’m finding this an extremely useful utility when developing databases to handle official surveys which are large scale and messy. It is only by working through the surveys you discover features such as a survey boost and then have to make a decision whether to break the table into a main and boost table (because only a smaller sub-sample answered the boost questions and to include these in the main table would include a lot of empty rows). One example of this is the Health Survey for England where there is a specific battery of questions which require input from a nurse and are applied to a smaller subset of the whole. For this reason, the database migration is closely linked to the data import and verification steps. I would really like to split them out, partly to increase my skill level is using Great Expectations a suite of “unit” style tests intended for data rather than code.\n","description":"Managing database migrations with sqitch","tags":["postgresql","sqitch","migration","version-control"],"title":"Managing migrations with sqitch","uri":"/tech/2021_02-16_sqitch/"},{"content":" From Pexels\nDavid Allen the consultant and author (sadly not Dave Allen the comedian) promotes a management system in his book “Getting Things Done: The Art of Stress Free Productivity”. In my opinion, there are two key features to this system. The first is learning to use a tracking system rather than carrying things in your head. The second is taking the time to review, prioritize and delete actions from this tracking system. So it isn’t just writing to-do lists because, because once you trust the process you don’t look at the to-do list - you only need to look at your schedule for a day. Less stress and worry, focus on finding flow for what is in front of you. If you trust your scheduling abilities, the only stress comes when you do your regular review, and delete, reschedule or add work to the system. There are some obvious overlaps with Agile and Waterfall, and it is no accident that Emacs has back-ends that allow org-mode to integrate with GitHub, BitBucket and GitLab to support a “Getting Things Done” workflow. And indeed, “Getting Things Done” encourages you in the review sessions to focus on what get a task done rather than adding tasks because you wish to enter a Stakhanovite competition for the world’s longest to-do list. “Getting Things Done” is sufficiently well established that there are numerous smartphone apps, such as “Don’t Forget the Milk” which go some way towards implementing it.\nHowever, to my mind there are some real bonus features you get when using Emacs org-mode to manage “Getting Things Done”. The first is time tracking. You can estimate the effort needed to complete a task, and clocking into a task is as simple as ctrl-x-i while clocking out is ctrl-x-o. This means as well as effort estimates, you can actually record the time spent on a task. I haven’t been quite so particular about this, but it is noticeable that the time tracking element certainly translated from Emacs to BitBucket if using Emacs org-mode to manage BitBucket tickets locally. I still find accurate time estimation difficult, and find it useful to be able to have a reasonably accurate measure of the time I have spent on tasks.\nThe one aspect I’m currently finding more difficult in “Getting Things Done” is that ad hoc tasks are now a more substantial part of my job. So this blog post will be rewritten over time as I get more comfortable developing a set of useful capture templates. The first things with any ad hoc activity I hit ctrl-c c which automatically clocks me into the capture task until I hit ctrl-c ctrl-c which takes the clock back to whatever I was doing before. So I have minimal administrative (time recording) interruptions to my original task, I just beam back when finished. The entry that I generate for this spontaneous ad hoc activity is stored in a dedicated file.\nsetq org-default-notes-file \"~/.gtd/refile.org\") (setq org-capture-templates (quote ((\"r\" \"respond\" entry (file \"~/git/org/refile.org\") \"* NEXT Respond to %:from on %:subject\\nSCHEDULED: %t\\n%U\\n%a\\n\" :clock-in t :clock-resume t :immediate-finish t) (\"m\" \"Meeting\" entry (file \"~/git/org/refile.org\") \"* MEETING with %? :MEETING:\\n%U\" :clock-in t :clock-resume t) (\"p\" \"Phone call\" entry (file \"~/git/org/refile.org\") \"* PHONE %? :PHONE:\\n%U\" :clock-in t :clock-resume t)))) If I wish to have a holistic understanding of where my work time goes, I will next need to set up some refile mechanism to move activities from this refile.org into more relevant files. At the moment, I’m guessing that these spontaneous activities will cost little time and I won’t wish to spend any more time categorizing them. But if particular patterns emerge, I will need to look into how refile works in org-mode. Again, this is largely about planning. Instead of trying to write a comprehensive scheduler, I can leave some carefully chosen generic templates and trust that my capture system will record enough information to be useful.\n","description":"Emacs, org-mode and getting things done","tags":["org-mode","Emacs","Getting-Things-Done","Project-Management"],"title":"GTD (Getting things done)","uri":"/tech/2021_02-08_gtd/"},{"content":" It’s been taking a little longer than I’d wanted to collate estimates of cycle activity based on the National Travel Survey, partly because there are big gaps in the 70s and 80s before it became a continuous survey, and partly because it is a complex survey.\nIn the meantime, I’ve produced a “simple” time series plot of the total number of serious and fatal cycle casualties reported to the police from 1979 to 2023. The biggest seasonal peak saw 36 males and 3 females killed or seriously injured on July 24th 1984 whereas the smallest seasonal peak seems to have been June 20th 2002 with 13 males killed or seriously injured. The plots give the impression that the numbers of injuries have been increasing since 2002.\nYou can zoom in and out these plots. But what is striking is that the peak for male fatal and seriously injured cyclists seems to happen in the summer. You would assume that this is a feature of exposure and not risk. The first assumption would be that there is more cycling that happens in the summer. You would not try to explain this pattern in terms of risk; indeed you would assume the risk to cyclists is greatest in the winter when the weather and light can be poor. It is also striking that in some years there was an October peak for female casualties, possibly coinciding with half term.\nThe next stages for this analysis will be to quantify the amount of cycling by time of year based on the National Travel Survey.\n","description":"Cyclists and seasonality","tags":["insights","road-collisions","official-statistics","stats19-data","age-period-cohort"],"title":"Interactive time series plots","uri":"/posts/2021-01-21_interactive_time_series/"},{"content":" I believe that data makes little sense without context. Often, we are lacking adequate information on context. When researching disease, knowing the age and sex of the patients is almost always important. When considering road injury research, it is harder to arrive at such an understanding because firstly there are both drivers and casualties to consider, and secondly there may be more than one driver. One of the most substantial weaknesses in the official road safety collision data are that no information is collected on uninjured passengers in any of the vehicles involved. However, one simple way of adding context is to consider trends over time. The so-called lexis diagram; a heatmap of injury rate by age and year lets us consider the age of a casualty, as well as the date on which an injury occurred. But, we could draw diagonal lines across the graph showing the birth year of the casualties. Indeed, we can extend this even further to a so-called age-period-cohort model which permits even fuller consideration of the data. However, for now, two simple Lexis diagrams are presented. The first shows the male motorcycle fatality rate per 100,000 population. It is clear that in the early 1980s, the fatality rate for males around 20 year old was extremely high, approaching 50 per 100,000 or around 1 in 2,000 males in that age group killed in a motorcycle crash. Bear in mind that it seems unlikely that more than half the males in this age group rode a motorcycle this is an extremely high fatality rate.\nThe second most striking thing about this plot is the cohort effect. A clear diagonal boundary can be extending from these males (born in the 1960s) which only starts to fade after 2010 by which time they have reached the age of 50. This is informative data. In the 2000s, a widely held belief in the road safety practitioner community was that there were a lot of deaths among “born again bikers” in their 40s, who had money, done minimal training and bought ultra-powerful motorbikes. Clearly a simple Lexis diagram cannot speak directly to this theory, but it doesn’t look consistent with it. Where is the dip in the 1990s in fatality rate among 30 year old male motorcyclists. It looks as if the 1960s born males were always likely to ride a bike and at risk of a fatal collision. By the time they were in their 40s, it seems a cultural bias “they should have known better” might have lead to the “born again biker” narrative, rather than the rather duller explanation that this age group were just more likely to ride a bike.\nBy way of a contrast, the second figure concerns the killed and seriously injured rate of males and females who were recorded injured as pedestrians at road crossings. There is no information on the presence of a road crossing prior to 1985 in the data. It appears there is a faint horizontal band (which narrows) consistent with secondary age pupils from 1985 to 2019. It appears broader at the start of the time horizon, and also note there are some faint blue squares. So the high injury rate seems to have lowered amongst younger children and teenagers. It is possible to claim that the rate has been increasing amongst older teenagers in the last few years.\nAnother striking feature of the plot is the high rates among older people in the late 1980s which has faded away. This raises lots of questions. Has there been a change in infrastructure, such as better provision of signalled crossings rather than zebra crossings? Or does this represent a bhavioural change, such as proportionately more older people using cars whereas previously they would have walked? Or might it be the case that older people are restricting their mobility for safety reasons?\nThere are two obvious follow ups to the points raised here. The first is to formally fit Age-Period-Cohort models. In the case of the motorcycle fatalities it will be of interest to examine the relationship between various motorcycle legislation (such as the introduction of Compulsory Basic Training) and the fatality rate. The second is to use data from the National Travel Survey to estimate rates relative to road usage rather than by population.\nData Notes.\nThe mid-year population estimates have been extracted from NOMIS. Some modelling was required to estimate population counts in 1979 and 1980. Moreover, some of the older data only has quinary data at the local authority level, which will require some modelling in order to impute values when extending this work to smaller geographical areas.\nThe publicly released data on data.gov.uk does not have individual ages for the oldest data. It has been possible to obtain individual ages from the data held in the Data Archive at Essex University from 1985, but Sprague polynomials have been used to estimate the individual ages prior to this date.\n","description":"Developing context: visualising the age and cohort (birth year) of road traffic casualties along with the date of their injury","tags":["insights","road-collisions","official-statistics","stats19-data","age-period-cohort"],"title":"Developing context; the value of historical data","uri":"/posts/2021-01-06_age_period_cohort/"},{"content":" Anonymous from Pinterest\nAfter some 12 years, I had to set up a new machine for my own use. I bought a reconditioned HP with around 16Gb RAM, 500Gb SSD. It came with a Windows OS, which I was going to wipe out. However, for various reasons (mainly some wonderful animation apps I use in teaching which require Mac or Windows only and need access to the sound system) I decided to keep the Windows host. First off, I decided to install my development machine under Vagrant/VirtualBox. I’m still fighting with VirtualBox (can’t get USB ext3 drives recognised, can’t get the sound working) and may end up having to dual boot. But apart from that, everything is wonderful. I suppose there’s a very minor inconvenience in having to logon to two computers (one real, one virtual) but that’s less hassle than dual booting.\nIt was possibly a little unfortunate that the new computer arrived close to Christmas. I therefore deluded myself into thinking I had plenty of time, so rather than just fire up my tried and tested dev box I thought I might upgrade the OS. I wasted a few hours trying to get Debian working - I could get a Debian server working but couldn’t get a KDE Desktop installed. As far as I can tell, this is related to Debian destroying the root account, but I decided I was just being a distro snob. So I reverted to Ubuntu. However (despite wasting a couple of hours wrangling Debian) I thought I should update my Ubuntu from 19.04 to 20.10. This is where the fun started.\nEventually, after a few hours of pure bafflement, I discovered that Vagrant insists on using ppa:ansible/ansible as its default-which-cannot-be-overridden repository. That was after spending quite a while trying to figure out why there were no ansible releases for Groovy Gorilla (at the time of writing, this is the current Ubuntu release 20.10). It turns out there are plenty of ansible releases for Groovy, in the Ubuntu repositories themselves. However, Vagrant forces you to use the ansible repositories and at time of writing they haven’t released anything newer than Eoan. A little bit of hunting around showed that you can install a python-only ansible using the pip installer. Unfortunately, that in turn caused a whole load of Python 2 stress. By default, amazingly, if you type python at a command line in Ubunutu Groovy you get Python 2. I couldn’t see a way of manipulating the playbook to reset the system Python to use Python 3. Eventually, I ended up with a very ugly hack. I provisioned using a shell script. The shell script installs ansible on the guest machine from the Ubuntu repositories and runs my playbook. This seems both hideously hacky and convoluted. My guess is that professionals use ansible from a server, and that using local provisioning system where you install ansible on the guest machine is rather a niche activity.\nAnyway, I’ve written this blog to remind me what an annoying few hours it was, and hopefully next year I will find some slack time on a train. I can read around and see what I was doing that made it all so messy.\n","tags":["Ansible","Configuration-management"],"title":"Some ansible woes","uri":"/tech/2020-12-18_ansible_woes/"},{"content":" Photo by Oleg Magni from Pexels\nI know this seems like a tiny problem compared to everything else that has happened in 2020. But I started the year having learnt how to use Travis CI. This meant I could put my hobby projects under some kind of continuous testing. Now, whatever the merits of various workflows that can used for CI, Travis was supported by the R community. It was well integrated into R package development. Unfortunately, after an acquisition and a restructure, Travis have recently announced that they are dropping the free tier for open source projects. I have no objection to paying for a service, and certainly suspect had I been using tools like Travis for the last 15 years I would have saved myself huge amounts of time and stress. It’s possibly even more important to have CI given my development habits, which go in fits and starts depending on other commitments. I can certainly remember needing to give a live demonstration and finding out that a package upgrade had broken all my code (and to make things worse, there was no question of using an earlier version of the software on the presentation machines). That said, there are plenty of developers who feel Travis have gone back on earlier promises regarding a free tier. Whatever the rights and wrongs, the bigger issue for me is that the R package building integration will likely move away from supporting Travis as thoroughly as it does just now.\nConsequently, I’ve just spent a few days setting up GitHub Actions. I made the slight mistake of taking the guidance from the GitHub docs directly. If I had looked at the R resources a little more carefully I would have seen that the usethis package contains a use_github_action_check_standard() function which sets everything up in your project/package folder including the yaml file needed to run GitHub actions. Anyway, following some trial and error, I now have a combination of GitHub community actions. GitHub actions check out the project repository and R-specific GitHub actions (available on GitHub itself) allow installation of an appropriate version of R. The R-Lib people also provide example workflows to conduct testing on Linux (Ubuntu), MacOS and Windows.\nI seemed to have my usual problem of needing to install some Ubuntu packages (header files to allow C++ compilation). I guess the answer to that really is to specify a docker image that can be used locally and run under GitHub actions, but that is something I will look into another day. For now, one of my use cases is testing code against the latest release of R, and seeing what breaks while I’ve been busy on other projects.\nIn terms of code coverage, after considerable effort fighting to persuade emacs to give me some coverage analysis, I find (of course) there is an off-the-shelf R package covr that does this beautifully. This package sends output to codecov.io. Yes, that’s built another free tier service into my tech stack. However, it produces beautiful summaries of code coverage as well as some gratuitous visualisations. The only current downside is that all the results are open to the world to see. That’s an incentive to plug some obvious holes in code coverage! It certainly buys time to try to get code coverage working as I want in emacs.\nAll-in-all, after two days of grouching, it looks as if GitHub actions is a perfectly viable CI tool for a simple open source project. My next challenge will be to see if this blog itself can be automatically deployed through a GitHub action whenever its source files are updated. There is a pre-canned example in R-Libs GitHub actions. Things may work much more smoothly if you fit in with community supported workflows.\n","description":"Migrating from Travis CI to Github actions ","tags":["CIDC","Github-actions"],"title":"Github actions","uri":"/tech/2020-12-14_github_actions/"},{"content":" David Sankel CppCon 2016 presentation “Building Software Capital”\nRecently, I’ve been updating my C++ skills. In the far past, I’ve used it to speed up algorithms in scripted languages. However, I have never used it as a standalone programming language. And when I say C++ I think I really mean C, I just happen to compile it with g++. Currently, I have a couple of problems which might change my limited exposure and get me much deeper in C++ code. It’s been interesting to see how many “problems” have been solved by some of the recent major upgrades to C++. Certainly last time I was using C++ I wasn’t using unit testing, although I did some testing informally from the calling language. However, I am now using Catch to do behaviour driven development. As part of the upskilling, as the evening goes on and I’m too tired to do any coding I’ve been watching some videos. The idea of the videos is to get a higher level picture of what has been going on in C++.\nDavid Sankel’s conference presentation (in the video linked above) is interesting even though it’s not that specifically tied to C++. It does however deal with a development frustration - the problem with tech debt. There is a powerful graph (which I’ve now seen several times) that argues in the early days of a software project, incurring technical debt affords a quicker route to market. It then does a great job of explaining why the compound interest on that debt accumulates exponentially fast. The result is that later it is made far slower to ship a product than if you had been addressing tech debt all along. And more constructively, it suggests what to do about this. There is also discussion at the end where some experienced developers share their history, and perhaps it needs to be noted at the time of this presentation David Sankel was working a Bloomberg. Many of the seven recommendations are widely accepted such as unit testing, code review and engineering infrastructure to enable building and system testing. However, one thing it does call for is documentation. This seems significant because many professionals claim that the “code is the documentation”. The counter argument is that in a large and complex software project, it is not feasible to read all the way down to the basic functions to understand what is going on. Well constructed and documented APIs should be all you need to work with. David therefore emphasises the importance of documenting contracts and how they add value. Firstly, to the developer of the underlying code, they serve as a specification for all the tests that are needed to ensure that code behaves as it should. To the developer who is going to call that API, it tells them all they need to know; they don’’t need to understand all the implementation details.\nO’Rly satire: R software has been exceptionally well documented from its launch\nWhat was so profoundly interesting to me is that I don’t think anyone would suggest that you learn R as a means of learning computer science. It lets you do amazing stuff with data, advanced models and visualisation. But it has many peculiarities which would rule it out as a place to study computational structures. However, in building R packages you are forced to write documentation for any public functions or datasets. So for all that might be quirky about R, developers are forced to document their contracts, and all users get to read the contracts. That doesn’t mean the software quality is universally high across all the R packages that exist. It’s just interesting that from the offset, R, which has always placed an importance on good scholarly documentation is a case study of how clearly documented contracts can be valued.\n","description":"Software capital: thoughts on contracts","tags":["software-capital"],"title":"Software capital","uri":"/tech/2020-12_01_api_contracts/"},{"content":" Photo by Christina Morillo from Pexels\nHaving worked in various Agile and Waterfall environments I have become fascinated by the potential mismatch between Data Modelling and Agile. Searching for some discussion pieces on this, I found an interesting post that reviews a conference discussion. Ultimately, this dates from Enterprise Data World 2015, but does seem to draw on a lot of expertise from enterprise level people who have had to reconcile Data Modelling and Agile. One of the statements made was that under Agile development: “There are numerous distinctions between the objectives, approaches, and needs of developers and data modelers that provide various points of conflict”. There is some exploration of the experience of various speakers with these conflicts.\nThis seems like an important topic, and while I think this answer draws attention to some of the issue, it doesn’t entirely address them. I’m sure that Data modelling doesn’t fit the “fail-fast-and-iterate” imperative of Agile. On the code developers side, you can increase the functionality, elegance, performance and even “pythonicness” of your code. Under Agile, you do this to meet a customer need and have an entire conceptual and infrastructural support structure to enable you to do this efficiently and safely. Even if substantial changes in code-base design are required, you will have accrued a good framework of unit tests to help you with any refactoring.\nHowever, if you get the data model wrong, inertia takes over. There may be some end-to-end integration tests which could reassure you that you have remodelled correctly. But I don’t believe there is a tight link between data model and code covered by a relevant level of testing. I’ve heard the statistical joke “if you beat the data hard enough it will confess to anything”. However, with a mis-specified data model you end up using your code to torture the data model to fit all the use cases.\nI have always been struck by a Linus Torvalds quote “Bad programmers worry about the code. Good programmers worry about data structures and their relationships”. So it’s not as if we are unaware that perhaps we are better at contemplating code quality absent a tight linkage to user data. This might give us elegant and performant code as measured by the tests we apply to it, but doesn’t necessarily translate to performant code in a real world application. This problem hasn’t been enough for substantial development work in refactoring tools that could guide us through a change of data model. It may be, for example, that many of our unit tests are wrong after we change our data model and need updating. Which ones? Even in this simple case, how many linters exist to help us identify the deprecated data structures in our unit tests.\nPreviously, I’d come to the conclusion that the answer had to be some kind of Waterfall style careful specification of the data models ahead of time. Some of the discussion reported from the Enterprise Data World 2015 conference seems to provide a degree of support to this; that Data Modellers need to be involved early in an Agile process. I’m not sure how that fits with an Agile process where you are developing customer requirements as you go; but then I wonder about the viability of developing a product with such a large set of unknowns that you are unable to even specify the strategic data model. However, the main conclusion seems to suggest that it should be possible (using branches for example) to try out variations in the data model and see if they solve a particular problem. After that, you trust your CI/CD to identify all the things that change breaks, and you can fix them. That still feels like a recipe for inertia. If the choice is between a good solution for your immediate problem, and a lot of fixing work in the rest of your code, or a mediocre solution to your immediate problem and no fixing work, which is going to win out under the time pressure of a sprint? As these mediocre solutions accumulate,at what point do you regret not making the change?\nMore profoundly, how can this work with a data pipeline, as used for statistical analysis or machine learning. There are numerous excellent tools (Airflow, Dagster) that help coordinate a complex data pipeline. The Data Model is far more than a well specified database (or warehouse, or lake). It’s a dynamic thing that doesn’t exist until the data pipeline is realised. How do we ensure we have optimal data models in this context? In conclusion, I don’t have the answers, and repeated web searching isn’t helping much. But I believe it is an important topic, and one I will revisit regularly.\n","description":"Does Agile worsen inertia though a failure to anticipate data models?","tags":["Agile","Management"],"title":"Conflicts between Agile and Data Modelling","uri":"/tech/2020-11-25_agile_and_data_modelling/"},{"content":" Photo by Christina Morillo from Pexels\nIn 1960, Eugene Wigner published “The unreasonable effectiveness of mathematics in the natural sciences”. Many commentators have indeed agreed it is surprising how relatively simple maths can indeed model so much of the natural world. Wigner argued that modelling a given scientific theory can lead to further advances in the theory. He also claimed success in that a mathematical model can make predictions which allow the theory to be empirically tested. Playing on the title of this paper, in 2009, researchers at Google presented “The unreasonable effectiveness of data”. This is often quoted by tech-evangelists and used to support the idea that “building models is expensive, training data is cheap”. I would argue it is not quite that simple.\nFirstly, it needs to be noted that Google were training Machine Learning (ML) algorithms in the context of language (translation). This was possible because they had accumulated vast quantities of training data. They had been digitising books at scale and therefore had a training set with corresponding human-translated material. So their translation algorithm does little to help us understand language or further our understanding of linguistics. It is still not capable of an artistic rendering of poetry from one language to another, where considerable leeway has to be taken to preserve structure and deeper meaning of poetry in different languages. It is also the case that language evolves. I would argue that there is at the very least a simple model underlying Google’s translation efforts - literary prose. And, as with a beginner learning about linear regression, I would also suggest that extrapolating beyond the range of your data could be extremely risky.\nOf course, Wigner also was immediately challenged over his arguments. Richard Hamming argued, for example that we tend to find whatever it is we are looking for, and also argued that we tend to create and select the mathematics we need to fit a particular situation. Perhaps more importantly, the nature that can be described by mathematics is only a limited part of the human experience. Significantly, many tech-evangelists have taken “The unreasonable effectiveness of data” beyond its context. Currently, image recognition is one of the hot topics in Machine Learning. Again, various cloud companies have acquired large image banks which make for great training data. Facebook for example has names and other data to associate with facial images. Again though, predictive use might well be limited to interpolation rather than extrapolation. Implicitly, there is a model here. The training set contains images of interest to those posting to Facebook.\nThe problem I’m alluding to are the ethical biases that arise when we have a biased data set. Facial recognition software is notoriously poor at recognising black faces. If we refute the use of models, we have no mechanism to correct this bias. This may not matter if your interest in Big Data is to increase sales, and you increase sales. But as public services become increasingly Big Tech-ified, there is a mindset gap here that needs to be filled.\n","description":"Unpacking the claim that data are cheaper than models","tags":["Big-Data","Ethics","Bias"],"title":"Unreasonable effectiveness of data","uri":"/posts/2020-11-16_unreasonable_effectiveness_of_data/"},{"content":" In total, over the four decades for which we have data (1979-2023), 574,593 fatalities have been reported to the police. Whilst it is the case that the number of reported fatalities has reduced considerably:\nIn 2023, 1,624 fatalities were reported. This still feels like a substantial tragedy and many argue there is considerable potential to reduce this much further if we took the decisions (such as limiting urban speeds) to make roads safer. The decline in road fatalities seems to have flattened since 2010. One of the contextual pieces of information we use to “justify” the level of road death has been that at least it is decreasing. This no longer seems to be the case. In medicine, it is standard to provide context to disease burden by estimating not just the number of deaths, but the numbers of years of life lost. An approximate method has been used to estimate this. Life expectancy tables have been taken from the ONS, and matched to the epoch, age and sex of each reported road fatality. For the year 1979 the life expectancy figures for 1980 were used. For collisions prior to 1985, individual ages were not available, only age bands and the mean age for each age band from 1985 was imputed as to provide an estimate of age prior to 1985. Consequently, an estimate of the total years of life lost due to road collisions can be estimated.\nIn total, over the four decades for which we have data (1979-2018), based on road collisions reported to the police we can estimate that 21,758,457 years of life have been lost. Again, we note that this number has been declining. But again, we note that in 2023, the most recent year for which we have data, 57,604.91 years of life were lost due to road collisions.\n","description":"Estimating years life lost due to road injury in GB","tags":["insights","road collisions","official statistics","stats19 data","life expectancy","years life lost"],"title":"GB Road Fatalities and Life Expectancy","uri":"/posts/2020-09-07-road_fatalities_life_expectancy/"},{"content":"One thing I have learnt is configuration management. It’s an under-rated aspect of reproducible research. I was quickly experimenting with an R package, but needed to install the sf package on my tinkering machine. That in turn required another R package units. And in turn, units needed some header files, as did sf. As it turns out, units package failed with a very nice error message telling me exactly what I had to do. And I had very had deja vu dealing with sf because I know I’ve handed that before.\nsudo apt install libgdal-dev libudunits2-dev One moral of the story is to remember not to try to do things quickly like this. The interesting thing is that this used to be the only way I worked. Cue lots of frustration when one package got updated and broke some other code.\nIt turns out that there is now a CRAN task view dedicated to repdroducible research which has a section on package reproducibility. This includes links to packages such as R bundler which attempts to tame your package requirements on a project by project basis.\nHowever, since having machines with enough RAM, I’ve found it very nice to use Virtual Machines during development. I used to use VirtualBox a long time ago (it let me run Linux on a MacBook). But you can use it with tools such as Vagrant to provision a virtual machine. And vagrant can in turn call an ansible script to provision this virtual machine. Voila, tinker away, break everything, start again.\ninstall.packages(\"XML\", repos = \"http://www.omegahat.net/R\") ","description":"Configuration management","tags":["Ansible","VirtualBox","Vagrant","Configuration-Management"],"title":"Configuration management","uri":"/tech/2020_09-03_ansible/"},{"content":"Something I’ve been droning on about for a few years concerns our reluctance to use datasets that already exist.\nSpecifically in relation to cycling, we have a lot of data on cycling that is rarely used, and that’s without worrying about data that aren’t disseminated in a way that lets us use it. Here are some high quality UK based data sources. These come from surveys that are very expensive to conduct:\nTravel to work data (Census, 2011). National Travel Survey (Ongoing and updated regularly) Active Lives Survey / Active People Survey (Ongoing and updated regularly) Other official surveys ask for information on cycling (this blog is really a placeholder for me to continue to list these). Local government has a lot of cycle loops in a lot of places, but seemingly there is no standard way to expose these on API to anyone who might be interested. Dock based bicycle hire (I’m sure some of these data are available) I’d be very interested if anyone can add to this list. Please do get in touch if you know of more. These data are presumably currently sitting around in data stores somewhere adding to our Carbon dioxide footprint without contributing as much as they could to our understanding of active transport. Now imagine not only having the data, but being able to apply some data fusion ideas so that we can gain a more holistic understanding and do this in a way that informs us where we have gaps in our knowledge, and quantify the value of closing those gaps.\n","description":"Data Sources and Cycling","tags":["Public-Data","Secondary-Analysis","Cycling","Government-Surveys"],"title":"Data Sources and Cycling","uri":"/posts/2020-09-03_data_sources_on_cyling/"},{"content":" Photo Credit: Photo by ThisIsEngineering from Pexels\nLegislation which carries large fines usually provides a good reason for checking out the law. In this case, the driver is the General Data Protection Regulation (GDPR), an EU directive which addresses the fundamental rights and freedoms of natural persons. Natural persons are “real” people, unlike llegal persons which includes entities like companies. It asserts that natural persons have the right to have their personal data personal data protected. It does not regulate data used for national security; nor does it cover data used for purely personal or household activity, such as sending birthdays cards.\nWhilst the definition of the data subject, a natural person (within the EU) limits the focus GDPR, the data are likewise limited. The directive only concerns data collected on “An identified or identifiable natural person”. Such a data subject can be regarded as “identified” within a certain group of people if he or she can be distinguished from all the other group members. To be specific, Article 4(1) of the GDPR states that a natural person is identifiable when it is possible to identify him or her, directly or indirectly. Some identifiers are obvious; name, identification number, some less obvious such as location. However, indirect identification may be possible through physical, physiological, genetic, mental, economic, cultural or social identity of that natural person. Therefore,\nArticle 9 specifies the following as sensitive personal data:\nRacial or ethnic origin Political opinions Religious or philosophical beliefs Trade union membership Genetic data Biometric data used to uniquely identify natural person Health data Data concerning individuals’ sex life Sexual orientation (It’s interesting that Sex is not listed, unless that is implied by Genetic data).\nThe GDPR is not law of itself. It applies to anyone working with data pertaining to EU citizens through action of member state’s law. The aim of the directive is to harmonise national law of member states. It superceded legislation on the protection of individuals with regard to the processing of their personal data as well as the free movement of such data (1995) on the one hand, and police and judicial co-operation on the other (2008). Proposals for GDPR were made in 2012 and aimed to protect individuals in terms of legislation around the processing of personal data by competent authorities for the purposes of prevention, investigation, detection, or prosecution of criminal offences or the enforcement of criminal penalties based on the free movement of these data. The directive entered force on 5th May 2016, with the requirement that member states implement this directive in their own law by 25th May 2018. It was signed into law by the UK and has not yet been removed from UK statute since departure from the EU. Whether the UK will remain abreast of changes to the GDPR over time remains to be seen. The Police and Criminal Justice Data Protection Directive was handled the same way at the same time.\nArticle 5 sets out six principles underlying the GDPR:\nPersonal data must be processed in a lawful, fair and transparent way. Article 8 mandates that parents or legal guardians have to consent for data processing on children. Data are to be collected only for specified, explicit and legitimate purposes. However, further processing for the purposes of the public interest, scientific or historical research or statistical purposes is not considered as incompatible with the initial purposes and is therefore allowed. Data must be adequate, relevant and limited to what is necessary in relation to the purposes for which they are processed. Data must be accurate, and where necessary, up to date. Data must not be kept in a form where it is possible to identify data subjects for any longer than necessary for the purposes of the processing. Appropriate security measures should be in place which includes protection against unauthorised or unlawful processing, destruction or damage. This can include encryption, authentication and authorisation mechanisms. Article 5 asserts that the data controller is responsible for compliance and demonstrating compliance. This begs the question as two what a “data controller is”. In order to operationalise the GDPR, a number of roles are defined:\nData controller Article 4(7) GDPR, a natural or legal authority who determines the purpose and method of processing personal data Data processor Article 4(8) GDPR, a natural or legal authority who processes data on behalf of controllers Data subject Article 4(1) GDPR, discussed above, a natural person Data Protection Officer (DPO) Articles 37-39 GDPR, a person designated by a controller and/or processor to ensure controllers, processors and their employees are following the GDPR and other relevant legislation Supervisory authorities Article 4(21) GDPR and Article 51 GDPR European Data Protection Board Article 68 GDPR So this is where it gets interesting. As a statistician we could be acting as controllers or (far more often) processors of data. But we could also be developing statistical software that is intended to be used by others. So one of the strong implications of the GDPR is that this process needs to be done in a way that respects the GDPR. Most specifically, this leads to the priciples of “Privacy by Design” An early statement on Privacy by Design was made by Ann Cavoukian (Canadian Information andn Privacy Commisioner) in 2011. The key ideas are:\nPrivacy by Design is a proactive approach aimed at preventing privacy risks rather than fixing them after problems arise. Privacy is a default setting. Privacy has to be embedded into design. It ensures full functionality and provides both privacy and security. Security is an integral systems for the whole lifecycle. “Privacy by Design” provides visibility and transparency. User interests and needs must be considered, and systems should be user-centric. And it can be noted that Article 25 of the GDPR makes more specific regulations around privacy by design.\n","description":"Seven principles of privacy by design","tags":["GDPR","Privacy by Design","Privacy"],"title":"Privacy by Design","uri":"/posts/2020-09-02_privacy_by_design/"},{"content":"The current iteration of this website is a static website based on Hugo. In my opinion, there is a lot to like about static websites. I have previously used Jekyll and once upon a time coded html manually. However, modern static website generators allow a wonderful presentation. The currently styling is based upon the Terrassa theme; for me it scored highly for accessibility and mobile friendliness. The bigger issue for me though is to do data and version controlling. With a dynamic website, you don’t necessarily know the data someone is accessing - it is drawn from a live database which by design can be updated independently of the website. With a static website, the data become version controlled along with the website. For some websites, that is a bad design choice. But I wanted to be able to demonstrate data analysis on data that I might revisit (because it has been updated) or using new methods. All this can be version controlled.\nThe “source” repository, and the “public” repository are hosted on Github. Github pages are great for Jekyll, so much so they will automatically build your sites from the source files. This is not the case with Hugo and currently it is necessary to split the site into a source repo containing a public sub-repo. It is this latter sub-repo which is pushed to the github pages. This public repo contains a .nojekyll file to stop Github trying to build pages.\nThere are a few software ecosystems which dominate professional data analytics. For modelling and analysis I prefer R along with PostgreSQL. We often need to access a number of data munging tools to get us from messy raw data to data which can be analysed, these can include Python, awk, sed. The R ecosystem is, in my opinion, much more complete in terms of critising, validating, verifying any modelling you are doing. It is also great when you need professional output. The R package blogdown is therefore used for this website. Literate programming has been around for a long time, Donald Knuth famously proposed this decades ago. Indeed Sweave has been a stable R feature for almost 20 years. Literate programming combines analysis code with report write up. Blogdown is an extension that uses (R) Markdown in the same way to produce .html files which can be included in a Hugo blog. Consequently, R/Blogdown/Hugo provides an environment for telling data stories which can copied, corrected, amended and version controlled to provide a full audit history.\nOne thing I have found recently (11th January 2021), I needed to add a piece to the yaml at the top of the .Rmd file. I’ve forgotten how I arranged to get the .Rmd files rendered before, but it seems it is now necessary to include this information.\noutput: blogdown::html_page: toc: true toc_depth: 1 number_sections: true fig_width: 6 With this in place, the standard blogdown::serve_site() will run the code in the .Rmd file and render it using pandoc as an html file which can be included in the website.\n","description":"Combining Hugo and R (using R-Blogdown)","tags":["Hugo","Static-Website","R","Blogdown"],"title":"Static Websites with Hugo and R-Blogdown","uri":"/tech/2020-08-18-hugo_blogdown/"},{"content":" In the event of a road collision involving at least one vehicle and one injury, there is a requirement in the UK to report to the police within 30 days. The data are called “STATs19” after the form on which the data are collected, and are made available as “Open Data” for anyone to analyse.\nThis immediately raises a number of points:\nWhat is a “road” collision (do car parks, driveways, private roads count)? What is an “injury”? More specifically, injury severity is coded by the police, with no medical input, as Fatal, Serious or Slight. What is the difference between “Serious” and “Slight” and how robust is this coding system. Fatal means anyone who dies within 30 days from their injuries. What is a vehicle (bicycles are included, but what about scooters - powered and un-powered)? More importantly, how well is this understood by those with a duty to report (the public) and those with a duty to record (the police) What is the police role when an injury collision is reported? Are they looking to see whether any offences have been committed or are they carrying out an in-depth investigation as to what factors may have lead to the collision? There are more detailed investigations for fatal and potentially fatal collisions, but the findings from these (e.g., coroners information on blood alcohol) are not necessarily passed on to the STATs19 data. In other words, when we analyse these STATs19 data we are not analysing road collisions. We are analysing an administrative process; police records of collisions which were reported to them. There is a huge gulf between the real world and these records. This does not imply malfeance; it is an inevitable result of any attempt to quantify the real world.\n## `summarise()` has grouped output by 'yw'. You can override using the `.groups` ## argument. A few things are clear from Figure 1.\nThere has been a general decline in the number of casualties over the four decades (with one exception, we will zoom on Serious collisions in Figure 2). There is day by day variation in the number of casualties (again, when we zoom in on Figure 2 it becomes very clear that there are annual seasonal trends that seem fairly regular). But interestingly, when we zoom in on serious casualties, we can see that the underlying trend seems to change in 2013; there seems to have been an increase in reported serious casualties overall since 2013. So, an obvious question is what is happening and why. A decrease in casualties sounds like a good thing; however if this is happening because less people are walking and cycling this is not a good thing - the health disbenefits from an inactive lifestyle might be more important than the lifes saved through injury reduction. It does however seem likely that road design standards have gradually improved since 1979 and certainly car safety has improved - from compulsory seat belt wearing through to air bags and safer cars. All of which sounds entirely reasonable, but begs the question: why has the number of seriously injured casualties been increasing since 2013. And there are many possible explanations:\nDue to increased automobile safety, less people are being killed in car crashes being seriously injured instead There was a decision made in 2013 to increase the quality processes around reporting injury severity, consequently many injuries that were previously reported as slight are now coded as serious As a result of austerity many more people are lift sharing (instead of taking a bus or train) and the number of casualties per collision has increased in cars. More people are reporting injury collisions There has been an increase in the number of people seriously injured as there has been an increase in the number of “vulnerable” road users (pedestrians and cyclists) With an aging population, we have people in cars who are more likely to incur a serious injury than a younger person who would only incur a slight injury Other: I am sure there are many other possibilities that could be considered. And there we are. The data can’t tell us anything actionable. There is no clear insight we can develop from these data, as presented here.\nWe could rule some suggestions in or out with closer examination of the data. But some suggestions seem very difficult to explore. In part, this is always a problem with observational data, especially when those data are a byproduct of an administrative system. But the aim of this blog is to revise and explore these questions.\n","description":"Overview of blog","tags":["insights","road-collisions","official-statistics","stats19-data"],"title":"Reported Road Injuries in GB","uri":"/posts/2020-08-07-stats19overview/"},{"content":" Photo credit: Vojtech Okenka from Pexels\nHaving worked as an academic, as a Data Scientist, as a Business Intelligence Specialist as well as doing a lot of consulting I have learnt a lot about the way we use data, and the way people perceive “statistics” (whatever that word really means).\nSometimes, a problem is so straightforward it only requires technical input (data processing) and it’s solved. More often though, a lot more interpretation is needed. Often, the problem is not so much about the “statistics” but about the study design. But plenty of times, people have seen some data and the problem is one of interpretation. People tend to prefer concise answers with no hedging. However, that is rarely possible. In part this is cultural; we have to accept that decisions need to be made in the absence of perfect knowledge. All we can do is to challenge some of the most egregious interpretative errors (and that is hard enough). Some of these fallacies are actually hard baked teachings in a lot of statistics text books and courses and it is so much harder to correct them after the event. Sometimes people approach me with an open mind; sometimes they have been so misleady about the nature of statistics that there is nothing I can do to help. However, sometimes, I found good answers to the problems and questions that got thrown my way. Sometimes, I got asked good questions I couldn’t answer. I have set up this blog as a reflective effort to try and summarise that experience and see if I can find any consistent themes.\n","description":"Overview of blog","tags":["insights","aims","statistics","data"],"title":"Insights from Data","uri":"/posts/2020-08-06-data_insights/"},{"content":" I suppose I started thinking about this really hard in the early days of the COVID-19 pandemic. I’m not sure it’s something we articulate well; the difference between a mathematical model. A mathematical model is meant to capture the science of a phenomenon in mathematical terms. A statistical model is meant to “fit” data and account for the uncertainty; but doesn’t necessarily explain the science. And there are certainly plenty of examples where you’re fitting a statistical model to estimate the parameters of a mathematical model. There’s a brilliant article from 2019 in Nature Communications which sets out the arguments in an advanced form. Here, I’m going to attempt to set out what I think are the key issues using more accessible (if slightly artificial) examples.\nOrdinary Linear Regression (a Taylor approximation)\nThe first example is a fictional bivariate data set. The figure below these data (blue dots) with a “line of best fit” (red). This is the basic building block of more realistic statistical modelling.\nBut what is this “line of best fit”? Bill Venables has pointed out that we can think of a standard linear regression as a Taylor approximation to the science, the mathematical functions that explain in scientific terms the mechanisms that lead from one variable to the other. If we assume \\(Y\\) is a random response variable, \\(\\boldsymbol(x) = (x_1, \\ldots x_p )\\) is a vector of \\(p\\) potentially explanatory variables and \\(Z\\) is a standard random normal variable. A conventional linear regression model is specified as:\n\\[Y \\approx \\beta_0 + \\sum_{i=1}^p \\beta_j (x_i - \\bar{x}_{i}) + \\sigma Z\\]\nIf we write a first order Taylor approximation we get:\n\\[Y \\approx f(\\bar{x}_0,0)+\\sum_{i=1}^p(\\bar{x}_0,0)(x_i-x_0) +f^{p+1}(\\bar{x}_0,0)Z\\]\nwhere the terms correspond. Note that we have assumed mean-centering; but in the absence of mean centering the additional terms would be wrapped into the intercept. We could extend this by adding first order interactions and single variable quadratics resulting in a second order Taylor approximation. So really, when we fit a statistical model, we are claiming that in the real world we have some function that relates one set of variables to the other:\n\\(Y = f(x, Z)\\)\nAs we don’t have the functional form of \\(f(\\cdot)\\), we claim we can still learn from a local approximation. Note the big caveat, locally approximate. Conversely, a mathematical model is meant to capture the science.\nLogistic growth model\nOne simple example of a mathematical model is the Logistic growth model. In the early stages it models expontial growth, but this slows as the population reaches the carrying capacity of its environment. At least that’s the science claimed by the model. Let \\(N\\) denote the size of a population and \\(t\\) denote time. I want to know how the size of the population changes with time, which we can write as \\(\\frac{dP}{dt}\\) (change in population size by change in time). The standard logistic growth model says that this rate of growth is equal to:\n\\(\\frac{dP}{dt} = rP\\left( 1 - \\frac{P}{K} \\right)\\)\nwhere \\(r\\) is the growth rate and \\(K\\) is the carrying capacity. The idea is that represents some fundamental state of nature. If I have an organism (a mould) and an environment (some left over Pizza) the mould will have an intrinsic rate of growth \\(r\\) in that environment and the Pizza will support a population of size \\(K\\). If we think a little about that formulae, at the start of our growth phase, \\(P\\) will be small relative to \\(K\\) so \\(\\frac{P}{K}\\) will be small (close to zero) and so the term inside the bracket will be close to \\(1\\). This tells us that the growth, the change in Population by time will be close to \\(rP\\). As the population gets close to the carrying capacity then \\(\\frac{P}{K}\\) will be close to \\(1\\). So the term inside the brackets will the close to \\(0\\) and the growth rate will be \\(r \\times P \\times\\) small value, i.e., it will be small. This is meant to describe a fundamental state of nature.\nAssumptions\nSo one massive and under-rated part of either paradigm of modelling, statistical or mathematical, is that both require us to make a large number of assumptions. If we have structured the math model correctly, it can make statements that are universally correct. If we have structured the statistical model in an acceptable way, we can make statements about relationships between observed variables. Both approaches to modelling have their place. But both models rely on assumptions. Neither model is innately true. Therefore the assumptions should be challenged and checked rigorously. I think this blog is an attempt to start thinking about assumption checking in models.\nTransport modelling\nSo the two examples given above are clearly toy examples. But these issues; the reliance on assumptions apply very much in models that are used in real life. Here is one attempt to explain some of the problems with the mathematical (economic) models used in transport modelling.\n","description":"Mathematical models and statistical models","tags":["Mathematical-models","Statistical-models","Model-validation","Model-criticism"],"title":"Mathematical models and statistical models","uri":"/posts/2020-08-12_math_or_stat/"},{"content":" A small trebuchet in action\nThis Trebuchet is a bit puny compared to the one at Warwick Castle. But it’s a fascinating piece of engineering, and certainly makes a bit of a difference from doing bisections with a compass. Here’s why I selected this as “the right tech” to use in school outreach work.\nThis is old work now, but I learnt a lot getting involved in it. We seem to have designed a school curriculum with the sole intended outcome of putting students off statistics for life. I assume that wasn’t done on purpose, but that is the effect. There seems to be a particular emphasis on teaching things that are easily assessed (can you do this calculation) and no real exploration of meaning. Nowadays, we are overloaded with observational data, but I’m nervous of using that. There is a lot more to consider. Why did we decide to measure that concept? Why did we choose to measure it that way? What are the strengths and weaknesses of the various ways of measuring? What are the limitations of using observational data? Consequently, I tried to work with experimental data; even if it was simple before/after studies because the use of experimental design just makes everything so much simpler. And of course this is where you hit a silo problem. How do you do experiments in a math class. Experiments are for other subjects. So I hit on the idea of using catapaults. They are using in Six-Sigma / Deming style trainings for captains of industry. So why can’t school kids benefits. Consequently I did a a number of masterclasses for the Royal Insitution , both in Plymouth at at the RI itself in London. I even got let loose one day in the main lecture theatre (the one where Faraday demonstrated electricity) with a big trebuchet firing rugby balls at people (and the very nice wall paper).\n","description":"The right tech for the job","tags":["Trebuchets","Statistics-learning"],"title":"The right tech for the job","uri":"/tech/2020-07-05_the_right_tech/"}]
